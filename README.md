# DeepSeek-R1-learning-note
### 摘要
DeepSeek-R1模型发布后，凭借其强大的长思维链推理能力，迅速在开源社区、学术界与产业界引起热烈反响。本文围绕DeepSeek-R1模型的核心技术展开系统性分析，旨在为研究者提供对DeepSeek-R1及推理型大语言模型技术的深入介绍。文章首先从实践角度梳理了DeepSeek-R1的部署方法与应用场景，继而重点剖析其创新架构中的关键技术：多头潜在注意力（MLA）、混合专家模型（MoE）、多Token预测（MTP）、群体相对策略优化（GRPO）强化学习策略等。随后，探讨了长思维链推理的实现逻辑和小模型知识蒸馏技术。最后，提出思考和总结。本文通过技术解读与思考，以期为后续研究和下游应用提供理论和实践的参考。
### 文章结构
本文第1章介绍了DeepSeek-R1的使用方法，并对其中的主要技术做了直观的介绍。如您只想最简单地定性了解其中的技术，仅阅读第1章即可。第2-5章是对以上主要技术的详细介绍，第6章介绍了DeepSeek-R1之后在推理型大模型的各类最新进展，并在第7章做出总结。
本文主要面向大语言模型垂直领域应用的研究者，主要目标是大家更方便地理解DeepSeek系列模型的核心技术，从而为研究提供一些参考和帮助。由于目标读者并非大语言模型的底层研究者，因此将不涉及大语言模型的底层架构和训练框架等内容（例如DeepSeek的FP8混合精度训练框架）。如您对这些细节感兴趣，可以参考所提供的相关链接和文献。
在阅读本文前，希望读者可以具有一定的深度学习基础（例如，了解transformer和注意力机制），并对大语言模型有初步的了解（例如，使用过ChatGPT或DeepSeek系列模型，了解思维链CoT和检索增强生成RAG等技术）。
### 致谢
本文的大部分内容及图表来源于DeepSeek技术报告、网络参考资料和论文文献，并加上一部分个人总结，文章最后提供了所参考的链接和文献列表，在此对这些内容的作者表示感谢。另外，感谢DeepSeek, Kimi和Grok3等大语言模型对本文写作的支持。鉴于个人能力有限，难免存在理解不全面甚至错误之处，敬请批评指正。
### 下载方式
您可直接下载文件DeepSeek-learning-note.pdf以获取笔记文档，同时在Attatch File文件夹中提供了使用大语言模型翻译对照的参考文献。
### 更新日志
- 2025-02-17: 第一版发布
- 2025-02-18: 修正错误，补充一些细节
- 2025-02-21: 补充一些细节，并增加DeepSeek最新成果NSA的解读
- 2025-02-23: 增加近期最新热点论文技术解读，将新技术部分单列为第6章，保持前文对R1技术的聚焦